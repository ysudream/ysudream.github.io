---
title: 오픈소스 AI, 괜찮을까?
feed: show
date: 09-04-2024
permalink: /opensourceai
---
![Markus Spiske from Unsplash](https://images.unsplash.com/photo-1533709752211-118fcaf03312?q=80&w=2940&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D)

*2024.04.14* - 기사로 쓰지 못하는 의식의 흐름 #1

<span style="color:red">인공지능(AI)의 오픈소스 개발, 생각보다 위험하지 않을까. </span>최근 리눅스 운영체제(OS)에 사용되는 오픈소스 압축 유틸리티 xz의 백도어 해킹 사태에 관한 [뉴스](https://news.hada.io/topic?id=14111)를 보고나서 든 생각. 전 세계 절반이 넘는 상업 서버가 리눅스를 [사용](https://www.redhat.com/en/blog/red-hat-leading-enterprise-linux-server-market#:~:text=Paid%20Linux%20subscriptions%20made%20up%20more%20than%2051%25)한다고. 조기에 누군가 악성코드를 발견했기에 망정이지, 자칫 주요 기업들의 서버가 해커의 침투에 무방비로 당할 뻔 했다.

걱정이 되는 건 오픈소스에 악성코드가 들어가게 된 계기. 오픈소스라도 다른 개발자들의 철저한 검증과 악성코드 자동화 검사를 거쳐야 코드를 심을 수 있다. 해커는 2021년부터 3년간 오픈소스 개발에 크고 작은 기여를 하며 신뢰를 얻었다. 그러다 모두 의심을 하지 않을 순간에 몰래 악성코드를 심어버린 것. 악성코드 자동화 검사도 어찌저찌 우회해서 피했다.

만약 오픈소스로 개발되는 AI에 이런 일이 일어났다면? 열심히 오픈소스로 AI 개발에 참여해온 익명의 개발자가, 어느 순간 돌변해 악성코드를 몰래 심는 일이 비현실적이진 않다. 긴 시간동안 신뢰를 쌓아 개인은 물론 기업과 정부가 애용하는 AI 프로그램이면 파급력은 더욱 커질 수 있지 않을까. 단순 백도어가 아니라, AI로 하여금 위해를 가하는 코드가 몰래 적용된다면...?

### 오픈소스의 한계

애초에 오픈소스 개발은 명확한 한계가 있다. AI 기술만 놓고보면, 모두에게 이 기술을 공개할 경우 인류에 위협이 될 것이란 주장은 꾸준히 나왔다. "핵무기를 오픈소스로 개발하드나" 류의 말이 대표적. 강력한 힘을 가진 AI 기술을 테러리스트 등이 악용할 수 있다는 우려다. AI의 '대부(Godfather)'인 제프리 힌튼 토론토대 교수가 이러한 걱정봇의[대표주자](https://www.joongang.co.kr/article/25159602#home). 돈과 명예를 버리고 "AI의 위험성을 알리기 위해" 구글에 사표를 던진 인물이다.

실제로 오픈소스에 악성코드를 심는 경우는 드문 일도 아니다. 오픈소스 소프트웨어 공급망 공격은 지난 1년 사이 3배가 늘었다는 [보고서](https://www.itworld.co.kr/news/309691)도 있다. 소프트웨어 공급망 공격이란, 기업이나 정부가 사용하는 소프트웨어에 몰래 백도어 같은 악성코드를 심는 해킹 기법. 이른바 'xz 백도어 사태'의 파급력이 컸을 뿐, 자잘하게 공격 시도는 꾸준히 늘어나고 있다. 오픈소스가 소프트웨어 공급망 공격을 더 쉽게 만든다는 [보고서](https://www.yna.co.kr/view/AKR20240117130700009)도 지난해에 나왔다.

게다가, 모두가 하나씩 힘을 합쳐 벽돌을 쌓는 오픈소스 개발은 태생적으로 '의존성 문제'가 있다. 한가지 오픈소스에서 개발한 코드를, 다른 오픈소스 프로그램이나 상업 프로그램이 자유롭게 가져다쓰기 때문이다. 그러다가 너가 개발한 오픈소스에 문제가 생기거나 악성코드가 심어졌을 때, 내 프로그램도 문제가 생긴다는 것. 웹 개발에 널리 적용된 유틸리티에 11줄의 코드가 삭제되자, 수 천 개의 프로그램이 먹통이 되는 [사태](https://www.bloter.net/news/articleView.html?idxno=22900)가 발생한 사례가 대표적이다.

### 악성코드, 결과를 모른다면

유독 AI 기술의 오픈소스 개발이 걱정되는 건 '블랙박스'라서다. 우리가 AI 모델이 어떻게 행동할지 예측할 수 없다는 걸 얘기한다. 극단적으로 말하면, AI 기술과 결합된 악성 코드 하나가 어떤 결과를 불러일으킬지 모두가 모른다는 것. 지금이야 기술의 수준이 낮아서 AI가 내놓을 위해가 제한적이라고는 한다. 그러나 미래에는? 전 세계가 코로나19로 멈추는 걸 아무도 내다보지 못했듯이, AI 악성코드로 전 세계가 멈출 수도 있다. 어떤 결과값을 내놓을지 모르는 작은 AI 악성코드 하나로 걷잡을 수 없이 전 세계 인터넷의 피해가 커지는 게 정말 공상소설일까.

'AI가 그정도로 파괴적일 수 있냐'를 두고선 학자들 사이에서도 의견이 갈린다. 힌튼은 인간을 파괴하는 AI가 나온다고 하고, 그와 함께 AI 4대천왕으로 불리는 얀 르쿤 NYU 교수는 아무리 시간이 지나도 AI는 멍청할거라고 한다. AI가 앞으로 어떻게 발전할지 국내 산업계와 학자들에게 물어보면? 대부분 모른다고 한다. 진짜 모르기 때문이다. 

### 숨어있는 돈의 논리, 그러나...

물론, AI의 오픈소스를 반대하는 것에는 돈의 논리가 숨어있는 것도 사실. 챗GPT로 한 순간에 세계적인 기업이 된 오픈AI도 원래는 비영리 기업이었다. 지금 와서 오픈소스로 개발하지 않고, 각종 AI 규제를 만들자고 하는 건 '사다리 걷어차기'라는 비판이 나오는 이유. 작년에 주구장창 능력치 만빵인 중앙일보 팩플팀 동료들이 쓴 기획도 이런 맥을 짚고자 했던 것.

그러나 연일 AI 개발이 '군비경쟁'으로 치닫고, AI 안전과 보안은 뒷전이 되는 걸 보니 조금씩 걱정이 몰려든다. 작년 영국에서 개최된 'AI 안전 정상회담'으로 과연 각국 정부가 변한 게 있을까. 우리는 AI 안전에 대해서 얼마나 알고 있을까.





